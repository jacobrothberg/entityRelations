{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from ast import literal_eval as make_tuple\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History \n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Embedding, Input,BatchNormalization, Dense, Bidirectional,LSTM,Dropout,Activation, concatenate\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Embedding, Input,InputLayer,BatchNormalization, Dense, Bidirectional,LSTM,Dropout,GRU,Activation\n",
    "from keras.backend import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMapping = {'Component-Whole(e2,e1)': 0, 'Other': 1, 'Instrument-Agency(e2,e1)': 2, 'Member-Collection(e1,e2)': 3, 'Cause-Effect(e2,e1)': 4, 'Entity-Destination(e1,e2)': 5, 'Content-Container(e1,e2)': 6, 'Message-Topic(e1,e2)': 7, 'Product-Producer(e2,e1)': 8, 'Member-Collection(e2,e1)': 9, 'Entity-Origin(e1,e2)': 10, 'Cause-Effect(e1,e2)': 11, 'Component-Whole(e1,e2)': 12, 'Message-Topic(e2,e1)': 13, 'Product-Producer(e1,e2)': 14, 'Entity-Origin(e2,e1)': 15, 'Content-Container(e2,e1)': 16, 'Instrument-Agency(e1,e2)': 17, 'Entity-Destination(e2,e1)': 18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./entityRelations/task2_train.csv')\n",
    "df2 = pd.read_csv('./entityRelations/train_dataset.csv')\n",
    "df_test = pd.read_csv('./entityRelations/task2_test.csv')\n",
    "df2_test = pd.read_csv('./entityRelations/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 19\n",
    "train_text = []\n",
    "p1 = [make_tuple(x)[0] for x in df2['entity_indices']]\n",
    "p2 = [make_tuple(x)[1] for x in df2['entity_indices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    split_sentence = df.iloc[i]['text'].split(' ')\n",
    "    prune_text = \" \".join(split_sentence[p1[i]:p2[i]+1])\n",
    "    train_text.append(prune_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9889 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=25000,lower=True,split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_seq = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 33\n"
     ]
    }
   ],
   "source": [
    "max_length = np.max([len(i) for i in train_sentence_seq])\n",
    "print(\"max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nsubjpobj', 'nsubjpasspobj', 'dobjpobj', 'nsubjdobj', 'pobjpobj',\n",
       "       'compoundpobj', 'attrpobj', 'compoundnsubj', 'conjpobj',\n",
       "       'pobjdobj', 'nsubjcompound', 'compounddobj', 'dobjdobj',\n",
       "       'nsubjnsubj', 'nsubjpunct', 'ROOTpobj', 'pobjcompound', 'attrdobj',\n",
       "       'nsubjpasscompound', 'dobjcompound', 'nsubjconj',\n",
       "       'compoundcompound', 'pobjnsubj', 'nsubjpassdobj', 'pobjconj',\n",
       "       'nsubjpasspunct', 'nsubjamod', 'compoundnsubjpass', 'punctprep',\n",
       "       'compoundattr', 'conjdobj', 'appospobj', 'pobjattr'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq = sequence.pad_sequences(train_sentence_seq, maxlen=max_length)\n",
    "dependency_list = df['deps']\n",
    "a, b = np.unique(dependency_list, return_counts=True)\n",
    "a_sorted = a[np.argsort(b)[::-1]]\n",
    "major_dep = a_sorted[:33]\n",
    "major_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_list_filter = []\n",
    "for dep in dependency_list:\n",
    "    if dep in major_dep:\n",
    "        dependency_list_filter.append(dep)\n",
    "    else:\n",
    "        dependency_list_filter.append(\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r',encoding='utf-8')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "1917494  words loaded!\n"
     ]
    }
   ],
   "source": [
    "w2vModel = loadGloveModel('glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV: 81\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 300\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "oov = 0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2vModel.keys():\n",
    "        embedding_matrix[i] = w2vModel[word]\n",
    "    else:\n",
    "        oov+=1\n",
    "print(\"OOV:\",oov)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "dependency_list_filter = lb.fit_transform(dependency_list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "\n",
    "p1_test = [make_tuple(x)[0] for x in df2_test['entity_indices']]\n",
    "p2_test = [make_tuple(x)[1] for x in df2_test['entity_indices']]\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    split_sentence = df_test.iloc[i]['text'].split(' ')\n",
    "    prune_text = \" \".join(split_sentence[p1_test[i]:p2_test[i]+1])\n",
    "    test_text.append(prune_text) \n",
    "\t\n",
    "test_sentence_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "x_test_seq = sequence.pad_sequences(test_sentence_seq, maxlen=max_length)\n",
    "\n",
    "dependency_list_test = df_test['deps']\n",
    "\n",
    "dependency_list_filter_test = []\n",
    "for dep in dependency_list_test:\n",
    "    if dep in major_dep:\n",
    "        dependency_list_filter_test.append(dep)\n",
    "    else:\n",
    "        dependency_list_filter_test.append(\"other\")\n",
    "\t\t\n",
    "dependency_list_filter_test = lb.fit_transform(dependency_list_filter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mut_ancestors_list = [make_tuple(x) for x in df['ancestors']]\n",
    "test_mut_ancestors_list = [make_tuple(x) for x in df_test['ancestors']]\n",
    "train_label = [labelMapping[ele] for ele in df2['labels']]\n",
    "train_label = dense_to_one_hot(np.array(train_label), 19)\n",
    "test_label = [labelMapping[ele] for ele in df2_test['labels']]\n",
    "test_label = dense_to_one_hot(np.array(test_label), 19)\n",
    "x_test_seq = sequence.pad_sequences(test_sentence_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return (sigmoid(x) * x)\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "\n",
    "def train_BiLSTM(x_train, ancestor_train, dep_train, y_train, \n",
    "                 x_val, ancestor_val, dep_val, y_val,\n",
    "                 embedding_matrix, max_length, max_features):\n",
    "    embedding_size = 300\n",
    "    batch_size = 64\n",
    "    epochs = 50\n",
    "    embedding_layer = Embedding(max_features,output_dim= embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    lstm0 = Bidirectional(LSTM(256,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(embedded_sequences)\n",
    "    lstm1 = Bidirectional(LSTM(128,activation=\"tanh\",dropout=0.2,return_sequences = True,\n",
    "                kernel_initializer='he_uniform'))(lstm0)\n",
    "    lstm2 = Bidirectional(LSTM(64,activation=\"tanh\",dropout=0.2,return_sequences = False,\n",
    "                kernel_initializer='he_uniform'))(lstm1)\n",
    "    bn1 = BatchNormalization()(lstm2)\n",
    "    \n",
    "    # other feature inputs \n",
    "    ancestor_input = Input(shape=(2,))\n",
    "    ancestor_feature = Dense(64, activation=swish)(ancestor_input)\n",
    "    \n",
    "    \n",
    "    dep_input = Input(shape=(34,))\n",
    "    dep_feature = Dense(128, activation=swish)(dep_input)\n",
    "    \n",
    "    combine_feature = concatenate([bn1, ancestor_feature, dep_feature])\n",
    "    dense1 = Dense(64, activation=swish)(combine_feature)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(32, activation=swish)(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense2)\n",
    "    preds = Dense(19, activation='softmax')(dropout2)\n",
    "    model = Model([sequence_input, ancestor_input, dep_input], preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    filepath = \"models/BiLSTM_3.hdf5\" \n",
    "    checkpoint = ModelCheckpoint(filepath,monitor='val_acc',save_best_only=True)\n",
    "    history = History()\n",
    "    callbacks_list = [checkpoint, history]\n",
    "    \n",
    "    history = model.fit([x_train, ancestor_train, dep_train], y_train, \n",
    "                        validation_data=([x_val, ancestor_val, dep_val], y_val), \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=callbacks_list)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 60s 479ms/step - loss: 2.3628 - acc: 0.3038 - val_loss: 2.1959 - val_acc: 0.4785\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 61s 487ms/step - loss: 1.7981 - acc: 0.4790 - val_loss: 1.4757 - val_acc: 0.6456\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 60s 481ms/step - loss: 1.4936 - acc: 0.5617 - val_loss: 1.0913 - val_acc: 0.6956\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 65s 520ms/step - loss: 1.2984 - acc: 0.6170 - val_loss: 1.0436 - val_acc: 0.7015\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 68s 545ms/step - loss: 1.1632 - acc: 0.6668 - val_loss: 0.9701 - val_acc: 0.7173\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 69s 549ms/step - loss: 1.0284 - acc: 0.7015 - val_loss: 0.9992 - val_acc: 0.7184\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 66s 532ms/step - loss: 0.9252 - acc: 0.7384 - val_loss: 1.0166 - val_acc: 0.7188\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 69s 549ms/step - loss: 0.8664 - acc: 0.7579 - val_loss: 1.0143 - val_acc: 0.7313\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 62s 495ms/step - loss: 0.7474 - acc: 0.7872 - val_loss: 1.0374 - val_acc: 0.7159\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 65s 519ms/step - loss: 0.6934 - acc: 0.8066 - val_loss: 1.0511 - val_acc: 0.7313\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 72s 577ms/step - loss: 0.6311 - acc: 0.8259 - val_loss: 1.0844 - val_acc: 0.7346\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 74s 595ms/step - loss: 0.5768 - acc: 0.8403 - val_loss: 1.0647 - val_acc: 0.7372\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 75s 600ms/step - loss: 0.5357 - acc: 0.8596 - val_loss: 1.1710 - val_acc: 0.7402\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 78s 625ms/step - loss: 0.4864 - acc: 0.8706 - val_loss: 1.3068 - val_acc: 0.7387\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 75s 598ms/step - loss: 0.4451 - acc: 0.8765 - val_loss: 1.2289 - val_acc: 0.7420\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 68s 547ms/step - loss: 0.4046 - acc: 0.8936 - val_loss: 1.4002 - val_acc: 0.7398\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 66s 527ms/step - loss: 0.3553 - acc: 0.9055 - val_loss: 1.3027 - val_acc: 0.7586\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 66s 527ms/step - loss: 0.3688 - acc: 0.9040 - val_loss: 1.2972 - val_acc: 0.7567\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 68s 544ms/step - loss: 0.3415 - acc: 0.9089 - val_loss: 1.4214 - val_acc: 0.7471\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 72s 573ms/step - loss: 0.2904 - acc: 0.9230 - val_loss: 1.3275 - val_acc: 0.7641\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 72s 575ms/step - loss: 0.2933 - acc: 0.9235 - val_loss: 1.5228 - val_acc: 0.7505\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 71s 567ms/step - loss: 0.2682 - acc: 0.9305 - val_loss: 1.4750 - val_acc: 0.7490\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 70s 562ms/step - loss: 0.2347 - acc: 0.9421 - val_loss: 1.6245 - val_acc: 0.7519\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 70s 561ms/step - loss: 0.2546 - acc: 0.9369 - val_loss: 1.6082 - val_acc: 0.7549\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 65s 520ms/step - loss: 0.2138 - acc: 0.9445 - val_loss: 1.8421 - val_acc: 0.7512\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 65s 518ms/step - loss: 0.2213 - acc: 0.9429 - val_loss: 1.7060 - val_acc: 0.7523\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 68s 547ms/step - loss: 0.2177 - acc: 0.9449 - val_loss: 1.7003 - val_acc: 0.7446\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 76s 608ms/step - loss: 0.2076 - acc: 0.9486 - val_loss: 1.7743 - val_acc: 0.7471\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 78s 625ms/step - loss: 0.1955 - acc: 0.9507 - val_loss: 1.8643 - val_acc: 0.7527\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 86s 691ms/step - loss: 0.1942 - acc: 0.9526 - val_loss: 1.8124 - val_acc: 0.7468\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 86s 688ms/step - loss: 0.1606 - acc: 0.9592 - val_loss: 1.8574 - val_acc: 0.7567\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 87s 694ms/step - loss: 0.1783 - acc: 0.9544 - val_loss: 1.8997 - val_acc: 0.7619\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 87s 697ms/step - loss: 0.1626 - acc: 0.9605 - val_loss: 1.9646 - val_acc: 0.7556\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 86s 687ms/step - loss: 0.1605 - acc: 0.9615 - val_loss: 1.9989 - val_acc: 0.7446\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 82s 658ms/step - loss: 0.1405 - acc: 0.9646 - val_loss: 2.0968 - val_acc: 0.7593\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 74s 596ms/step - loss: 0.1432 - acc: 0.9641 - val_loss: 2.2864 - val_acc: 0.7446\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 73s 582ms/step - loss: 0.1687 - acc: 0.9606 - val_loss: 1.9750 - val_acc: 0.7549\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 82s 655ms/step - loss: 0.1425 - acc: 0.9689 - val_loss: 1.9694 - val_acc: 0.7549\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 72s 577ms/step - loss: 0.1378 - acc: 0.9686 - val_loss: 2.0549 - val_acc: 0.7611\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 79s 631ms/step - loss: 0.1324 - acc: 0.9691 - val_loss: 1.9960 - val_acc: 0.7541\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 83s 661ms/step - loss: 0.1522 - acc: 0.9657 - val_loss: 2.0219 - val_acc: 0.7556\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 77s 613ms/step - loss: 0.1219 - acc: 0.9725 - val_loss: 2.1767 - val_acc: 0.7486\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 78s 626ms/step - loss: 0.1119 - acc: 0.9745 - val_loss: 2.0250 - val_acc: 0.7556\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 77s 614ms/step - loss: 0.1190 - acc: 0.9725 - val_loss: 2.1622 - val_acc: 0.7508\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 75s 602ms/step - loss: 0.1130 - acc: 0.9739 - val_loss: 2.1869 - val_acc: 0.7582\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 74s 589ms/step - loss: 0.1126 - acc: 0.9715 - val_loss: 2.1527 - val_acc: 0.7416\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 76s 607ms/step - loss: 0.1041 - acc: 0.9726 - val_loss: 2.2874 - val_acc: 0.7545\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 81s 652ms/step - loss: 0.1014 - acc: 0.9770 - val_loss: 2.4475 - val_acc: 0.7519\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 82s 658ms/step - loss: 0.0966 - acc: 0.9754 - val_loss: 2.4223 - val_acc: 0.7556\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 81s 652ms/step - loss: 0.0996 - acc: 0.9758 - val_loss: 2.2580 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model, history = train_BiLSTM(np.asarray(x_train_seq), np.asarray(train_mut_ancestors_list), np.asarray(dependency_list_filter), np.asarray(train_label), \n",
    "                     np.asarray(x_test_seq), np.asarray(test_mut_ancestors_list), np.asarray(dependency_list_filter_test), np.asarray(test_label),\n",
    "                     embedding_matrix,\n",
    "                     max_length,\n",
    "                     len(word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "prediction = model.predict([np.asarray(x_test_seq), np.asarray(test_mut_ancestors_list), np.asarray(dependency_list_filter_test)], batch_size = 1000)\n",
    "class_pred = np.argmax(predict1ion, axis=1)\n",
    "class_true = np.argmax(test_label, axis=1)\n",
    "conf = confusion_matrix(class_true, class_pred)\n",
    "precision_recall_fscore = precision_recall_fscore_support(class_true, class_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
