{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "1917494  words loaded!\n"
     ]
    }
   ],
   "source": [
    "from CorpusReader import *\n",
    "from FeatureExtractor import *\n",
    "import pandas as pd\n",
    "from DataTransform import transform\n",
    "from LSTM import train_model\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMappings = {'Other': 0,'Instrument-Agency(e1,e2)': 1, 'Instrument-Agency(e2,e1)': 2,\n",
    "                     'Cause-Effect(e1,e2)': 3, 'Cause-Effect(e2,e1)': 4,'Member-Collection(e1,e2)': 5,\n",
    "                     'Member-Collection(e2,e1)' : 6,'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
    "                     'Content-Container(e1,e2)': 9, 'Content-Container(e2,e1)': 10,'Message-Topic(e1,e2)': 11,\n",
    "                     'Message-Topic(e2,e1)': 12, 'Product-Producer(e1,e2)': 13,'Product-Producer(e2,e1)': 14,\n",
    "                     'Entity-Origin(e1,e2)': 15, 'Entity-Origin(e2,e1)': 16,'Component-Whole(e1,e2)': 17,\n",
    "                     'Component-Whole(e2,e1)': 18}\n",
    "\n",
    "classToLabel = {0: ('Other','NA'),1: ('Instrument-Agency','(e1,e2)'), 2: ('Instrument-Agency','(e2,e1)'),\n",
    "                    3: ('Cause-Effect','(e1,e2)'),4: ('Cause-Effect','(e2,e1)'),5: ('Member-Collection','(e1,e2)'),\n",
    "                    6: ('Member-Collection','(e2,e1)'),7: ('Entity-Destination','(e1,e2)'),8: ('Entity-Destination','(e2,e1)'),\n",
    "                    9: ('Content-Container','(e1,e2)'),10: ('Content-Container','(e2,e1)'),11: ('Message-Topic','(e1,e2)'),\n",
    "                    12: ('Message-Topic','(e2,e1)'),13: ('Product-Producer','(e1,e2)'),14: ('Product-Producer','(e2,e1)'),\n",
    "                    15: ('Entity-Origin','(e1,e2)'),16: ('Entity-Origin','(e2,e1)'),17: ('Component-Whole','(e1,e2)'),\n",
    "                    18: ('Component-Whole','(e2,e1)')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR_train = CorpusReader()\n",
    "dataset_train = CR_train.read('semeval_train.txt')\n",
    "featureExtractor_train = FeatureExtractor()\n",
    "new_dataset_train = featureExtractor_train.getFeatures(dataset_train)\n",
    "CR_test = CorpusReader()\n",
    "dataset_test = CR_test.read('semeval_test.txt')\n",
    "featureExtractor_test = FeatureExtractor()\n",
    "new_dataset_test = featureExtractor_test.getFeatures(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = [labelMappings[ele] for ele in dataset_train['labels']]\n",
    "label_train = dense_to_one_hot(np.array(label_train), len(labelMappings))\n",
    "x_label = np.asarray(label_train)\n",
    "label_test = [labelMappings[ele] for ele in dataset_test['labels']]\n",
    "label_test = dense_to_one_hot(np.array(label_test), len(labelMappings))\n",
    "y_label = np.asarray(label_test)\n",
    "tokenizer,embedding_matrix, max_length, major_dep, word_index, x_text_seq, x_mut_ancestors_list = transform(labelMappings,new_dataset_train,train = True)\n",
    "(_,_,_,_,_,y_text_seq,y_mut_ancestors_list) = transform(labelMappings,new_dataset_test,tokenizer,max_length,major_dep,word_index,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_list = new_dataset_train['dependents']\n",
    "a, b = np.unique(dependency_list, return_counts=True)\n",
    "a_sorted = a[np.argsort(b)[::-1]]\n",
    "major_dep = a_sorted[:33]\n",
    "major_deps = {}\n",
    "i = 0\n",
    "for j in range(33):\n",
    "    major_deps[major_dep[j]] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Component-Whole', '(e2,e1)')\n",
      "\n",
      " total time taken :  2.626636505126953\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "s = \"The opening and closing of the <e1>heart</e1> <e2>valves</e2> produce the sound of the heartbeat.\"\n",
    "df_sent = read_sentence(s)\n",
    "featureExtractor_sent = FeatureExtractor()\n",
    "new_data_sent = featureExtractor_sent.getFeatures(df_sent)\n",
    "(_,_,_,_,_,s_text_seq,s_mut_ancestors_list) = transform(labelMappings,new_data_sent,tokenizer,max_length,major_dep,word_index,train=False)\n",
    "dependency_list = new_data_sent['dependents']\n",
    "s_dependency = dependency_encoder(major_deps,dependency_list)\n",
    "prediction = model.predict([s_text_seq,s_mut_ancestors_list,s_dependency],batch_size=1)\n",
    "class_pred = np.argmax(prediction, axis=1)\n",
    "print(classToLabel[class_pred[0]])\n",
    "t1 = time.time()\n",
    "print(\"\\n total time taken : \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " total time taken :  34.083810329437256\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "CR_test = CorpusReader()\n",
    "dataset_test = CR_test.read('semeval_test.txt')\n",
    "featureExtractor_test = FeatureExtractor()\n",
    "new_dataset_test = featureExtractor_test.getFeatures(dataset_test)\n",
    "label_test = [labelMappings[ele] for ele in dataset_test['labels']]\n",
    "label_test = dense_to_one_hot(np.array(label_test), len(labelMappings))\n",
    "test_dependency_list = new_dataset_test['dependents']\n",
    "y_dependency_list_filter = dependency_encoder(major_deps,test_dependency_list)\n",
    "y_label = np.asarray(label_test)\n",
    "(_,_,_,_,_,y_text_seq,y_mut_ancestors_list) = transform(labelMappings,new_dataset_test,tokenizer,max_length,major_dep,word_index,train=False)\n",
    "prediction = model.predict([y_text_seq,y_mut_ancestors_list,y_dependency_list_filter],batch_size=1000)\n",
    "t1 = time.time()\n",
    "print(\"\\n total time taken : \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  71.71103576526859 \n",
      "recall:  71.95760891693234 \n",
      "fscore:  71.60816071745583\n",
      "Correct relation,correct edges:  75.26683842473317\n",
      "Correct relation, wrong edge:  0.9569377990430622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\poorn\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_pred = np.argmax(prediction, axis=1)\n",
    "class_true = np.argmax(y_label, axis=1)\n",
    "conf = confusion_matrix(class_true, class_pred)\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(class_true, class_pred, average='macro')\n",
    "print(\"precision: \",precision*100,\"\\nrecall: \",recall*100,\"\\nfscore: \",fscore*100)\n",
    "y_true = [classToLabel[x] for x in class_true]\n",
    "y_pred = [classToLabel[x] for x in class_pred]\n",
    "cor_rel_cor_edge, cor_rel_wr_edge = 0, 0\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i][0] == y_pred[i][0]:\n",
    "        if y_true[i][1] == y_pred[i][1]:\n",
    "            cor_rel_cor_edge += 1\n",
    "        else:\n",
    "            cor_rel_wr_edge += 1\n",
    "print(\"Correct relation,correct edges: \",100 * cor_rel_cor_edge/ len(y_true))\n",
    "print(\"Correct relation, wrong edge: \", 100 * cor_rel_wr_edge / len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
